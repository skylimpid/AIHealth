############################################################
# Please add loss and accuracy records here at each checkin if the tuning related changes are made.
# Please do not git push if the change makes worse in optimization or record here for reference purpose
#
# anaconda3/bin/python AIHealth/Training/train_detector.py
############################################################

###
dropout = tf.layers.dropout(feat, rate=0.4, name="dropout_feature")  
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.01
__C.TRAIN.BATCH_SIZE =  5


Step: 10, Avg Loss: 0.793708, Cur Loss: 0.793708
Epoch 1 finished on 1448 batches in avg loss 0.770019 and cur loss 0.764823.
Epoch 2 finished on 1448 batches in avg loss 0.756032 and cur loss 0.752899.
Epoch 3 finished on 1448 batches in avg loss 0.742929 and cur loss 0.733387.


Epoch 9 finished on 1448 batches in avg loss 0.610775 and cur loss 0.572691.
Epoch 10 finished on 1448 batches in avg loss 0.603525 and cur loss 0.609269.
Epoch 11 finished on 1448 batches in avg loss 0.593453 and cur loss 0.591157.


###
dropout = tf.layers.dropout(feat, rate=0.4, name="dropout_feature")
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.9
__C.TRAIN.BATCH_SIZE = 10

Step: 10, Avg Loss: 0.787539, Cur Loss: 0.787539
Epoch 1 finished on 1448 batches in avg loss 0.773786 and cur loss 0.770592.
Epoch 2 finished on 1448 batches in avg loss 0.758528 and cur loss 0.758167.
Epoch 3 finished on 1448 batches in avg loss 0.749668 and cur loss 0.744524.

Epoch 5 finished on 1448 batches in avg loss 0.734460 and cur loss 0.733835.

Epoch 8 finished on 1448 batches in avg loss 0.673954 and cur loss 0.659271.
Epoch 9 finished on 1448 batches in avg loss 0.651847 and cur loss 0.646407.
Epoch 10 finished on 1448 batches in avg loss 0.632067 and cur loss 0.624173.
Epoch 11 finished on 1448 batches in avg loss 0.620530 and cur loss 0.632186.


###
dropout = tf.layers.dropout(feat, rate=0.4, name="dropout_feature")
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.01
__C.TRAIN.BATCH_SIZE = 10

Step: 10, Avg Loss: 0.787745, Cur Loss: 0.787745
Epoch 1 finished on 1448 batches in avg loss 0.775415 and cur loss 0.768271.
Epoch 2 finished on 1448 batches in avg loss 0.763157 and cur loss 0.755870.
Epoch 3 finished on 1448 batches in avg loss 0.749217 and cur loss 0.749444.


###
Remove all loss functions except classify_loss_with_pos_neg_with_hard_mining
dropout = tf.layers.dropout(feat, rate=0.4, name="dropout_feature")
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.01
__C.TRAIN.BATCH_SIZE = 10

Epoch 1 finished on 1448 batches in avg loss 0.773969 and cur loss 0.762390.
Epoch 2 finished on 1448 batches in avg loss 0.756873 and cur loss 0.749570.

Epoch 8 finished on 1448 batches in avg loss 0.665592 and cur loss 0.651356.
Epoch 9 finished on 1448 batches in avg loss 0.638862 and cur loss 0.649554.
Epoch 10 finished on 1448 batches in avg loss 0.618648 and cur loss 0.629125.

Epoch 19 finished on 1448 batches in avg loss 0.568994 and cur loss 0.561298.
Epoch 20 finished on 1448 batches in avg loss 0.563642 and cur loss 0.560136.

Epoch 30 finished on 1448 batches in avg loss 0.548439 and cur loss 0.551291.
Epoch 31 finished on 1448 batches in avg loss 0.544557 and cur loss 0.540299.

Epoch 37 finished on 1447 batches in avg loss 0.537473 and cur loss 0.525422.
Epoch 38 finished on 1448 batches in avg loss 0.537316 and cur loss 0.530574.
Epoch 39 finished on 1448 batches in avg loss 0.533906 and cur loss 0.532312.

###
__C.TRAIN.LEARNING_RATE_STEP_SIZE = 2000
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.95
Step: 10, Avg Loss: 0.784183, Cur Loss: 0.784183
Step: 20, Avg Loss: 0.778457, Cur Loss: 0.772731
Epoch 1 finished on 1447 batches in avg loss 0.773498 and cur loss 0.777148.
Epoch 2 finished on 1448 batches in avg loss 0.764426 and cur loss 0.755934.
Epoch 30 finished on 1448 batches in avg loss 0.535807 and cur loss 0.547894.
Epoch 83 finished on 1447 batches in avg loss 0.516954 and cur loss 0.522610.
Epoch 84 finished on 1448 batches in avg loss 0.516770 and cur loss 0.515758.
Epoch 85 finished on 1448 batches in avg loss 0.517570 and cur loss 0.510377.

###
Use single classify_loss_with_pos_neg_with_hard_mining only
__C.TRAIN.LEARNING_RATE_STEP_SIZE = 2000
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.95
Step: 20, Avg Loss: nan, Cur Loss: 0.783454
Epoch 1 finished on 1448 batches in avg loss nan and cur loss 0.765568.
Epoch 48 finished on 1448 batches in avg loss nan and cur loss 0.525415.
Step: 10, Avg Loss: 0.541066, Cur Loss: 0.541066

###
Use single classify_loss_with_pos_neg_with_hard_mining only
__C.TRAIN.LEARNING_RATE_STEP_SIZE = 2000
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.95
But update in Detector_Net_Loss.py as follows:
        #neg_prob = tf.nn.sigmoid(neg_output)
        neg_prob = neg_output
Step: 10, Avg Loss: 0.677721, Cur Loss: 0.677721
Epoch 1 finished on 1448 batches in avg loss 0.625859 and cur loss 0.595074.
Epoch 2 finished on 1448 batches in avg loss nan and cur loss 0.420814.
Epoch 9 finished on 1448 batches in avg loss 0.369015 and cur loss 0.367769.
Epoch 10 finished on 1448 batches in avg loss 0.373300 and cur loss 0.365022.

###
__C.TRAIN.LEARNING_RATE_STEP_SIZE = 2000
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.95
But update in Detector_Net_Loss.py to remove sigmoid in two places as follows:
        #neg_prob = tf.nn.sigmoid(neg_output)
        neg_prob = neg_output
Step: 10, Avg Loss: 0.648994, Cur Loss: 0.648994
Epoch 1 finished on 1448 batches in avg loss 0.599607 and cur loss 0.498883.
Epoch 2 finished on 1448 batches in avg loss 0.428982 and cur loss 0.401546.
Epoch 3 finished on 1448 batches in avg loss 0.393580 and cur loss 0.390773.
Epoch 10 finished on 1448 batches in avg loss 0.369563 and cur loss 0.364238.
Epoch 19 finished on 1448 batches in avg loss 0.309773 and cur loss 0.319367.
Epoch 22 finished on 1448 batches in avg loss 0.276673 and cur loss 0.277951.

###
Use Loss_2 version
__C.TRAIN.LEARNING_RATE = 0.1
__C.TRAIN.LEARNING_RATE_STEP_SIZE = 2000
__C.TRAIN.LEARNING_RATE_DECAY_RATE = 0.95
Batch: 10, Avg Loss: 0.716349, Cur Loss: 0.716349
Batch: 20, Avg Loss: 0.711590, Cur Loss: 0.706832
Batch: 30, Avg Loss: 0.709577, Cur Loss: 0.705550
Batch: 40, Avg Loss: 0.707931, Cur Loss: 0.702994
Batch: 50, Avg Loss: 0.706562, Cur Loss: 0.701084
Epoch 1 finished on 290 batches in avg loss 0.698102.
Epoch 2 finished on 290 batches in avg loss 0.689795.
Epoch 3 finished on 290 batches in avg loss 0.659623.
Epoch 10 finished on 290 batches in avg loss 0.361587.
Epoch 15 finished on 290 batches in avg loss 0.276596.
Batch: 200, Avg Loss: 0.292803, Cur Loss: 0.258912
Batch: 210, Avg Loss: 0.291338, Cur Loss: 0.262038
Batch: 220, Avg Loss: 0.290432, Cur Loss: 0.271413
Batch: 230, Avg Loss: 0.292155, Cur Loss: 0.330059
Batch: 240, Avg Loss: 0.290219, Cur Loss: 0.245695
Batch: 250, Avg Loss: 0.289020, Cur Loss: 0.260245

###
Use Loss first version
Step: 10, Avg Loss: 0.647705, Cur Loss: 0.647705
Epoch 1 finished on 1448 batches in avg loss 0.604641 and cur loss 0.528300.
Epoch 2 finished on 1447 batches in avg loss 0.432919 and cur loss 0.397833.
Epoch 4 finished on 1448 batches in avg loss 0.378208 and cur loss 0.376583.
Epoch 10 finished on 1448 batches in avg loss 0.289488 and cur loss 0.241632.
Epoch 20 finished on 1448 batches in avg loss 0.233479 and cur loss 0.211144.
Epoch 30 finished on 1448 batches in avg loss 0.212138 and cur loss 0.202847.
Epoch 40 finished on 1448 batches in avg loss 0.197894 and cur loss 0.196362.
Epoch 50 finished on 1448 batches in avg loss 0.189050 and cur loss 0.182118.
Epoch 60 finished on 1448 batches in avg loss 0.189558 and cur loss 0.323245.
Epoch 70 finished on 1448 batches in avg loss 0.183797 and cur loss 0.177723.
Epoch 80 finished on 1448 batches in avg loss 0.178727 and cur loss 0.171339.
Epoch 98 finished on 1448 batches in avg loss 0.178775 and cur loss 0.172377.
Epoch 99 finished on 1448 batches in avg loss 0.178105 and cur loss 0.170557.
Epoch 100 finished on 1448 batches in avg loss 0.177680 and cur loss 0.214867.
The total time used in training: 87498.74522042274 seconds (24.3 hours)

#####
Run instance.predict in train_detector
Start to predict user:ff8599dd7c1139be3bad5a0351ab749a
(792, 24, 24, 24, 3, 5)
Predict user:ff8599dd7c1139be3bad5a0351ab749a spend: 55.559577226638794
start to post-process the predict result for user:ff8599dd7c1139be3bad5a0351ab749a
finish the post-process for user:ff8599dd7c1139be3bad5a0351ab749a spend:0.0908350944519043
total process time:87970.95692634583